import logging
import requests
from datetime import datetime
from zoneinfo import ZoneInfo
from app.models import CallbackData
from app.engine.dynamic_fetcher import fetch_dynamic
from app.engine.static_fetcher import fetch_static
from app.parser.ai_parser import parse_with_ai

LOG = logging.getLogger(__name__)
TIMEZONE = ZoneInfo("Asia/Seoul")

async def run(event):
    LOG.info("ğŸš€ ì§€ëŠ¥í˜• í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    request_site_name = event.get("siteName") or "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜"
    target_urls = event.get("targetUrls") or [event.get("targetUrl")]
    user_profile = event.get("userProfile", {})
    user_id = event.get("userId")
    
    all_notices = []

    for url in target_urls:
        if not url: continue
        
        # 1. ë™ì  ìˆ˜ì§‘ ì‹œë„ (Playwright)
        content = await fetch_dynamic(url)
        
        # 2. ì‹¤íŒ¨ ì‹œ ì •ì  ìˆ˜ì§‘ ì‹œë„ (Requests)
        if not content or len(content) < 100:
            LOG.warning(f"âš ï¸ ë™ì  ìˆ˜ì§‘ ì‹¤íŒ¨, ì •ì ìœ¼ë¡œ ì „í™˜: {url}")
            content = fetch_static(url)

        if not content:
            LOG.error(f"âŒ ëª¨ë“  ìˆ˜ì§‘ ìˆ˜ë‹¨ ì‹¤íŒ¨: {url}")
            continue

        # 3. AI ë²”ìš© íŒŒì‹± (Gemini 2.0)
        notices = parse_with_ai(content, url, user_profile)
        
        for n in notices:
            # AIê°€ ì¤€ ë°ì´í„°ì—ì„œ ë§í¬ë¥¼ ì°¾ê¸° ìœ„í•´ ì—¬ëŸ¬ í‚¤ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
            link = n.get("link") or n.get("url") or n.get("originalUrl") or n.get("original_url")
            
            if not link:
                LOG.warning(f"âš ï¸ ê³µì§€ì‚¬í•­ì—ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤: {n.get('title')}")
                continue

            all_notices.append({
                "title": n.get("title") or "ì œëª© ì—†ìŒ",
                "summary": n.get("summary") or "ìš”ì•½ ì—†ìŒ",
                "originalUrl": str(link),  # í™•ì‹¤í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
                "sourceName": "ì§€ëŠ¥í˜• í¬ë¡¤ëŸ¬",
                "category": n.get("category") or "ì¼ë°˜",
                "sourceName": request_site_name,
                "relevanceScore": float(n.get("score", 0.0)),
                "timestamp": datetime.now(TIMEZONE).isoformat()
            })

    # 4. ê²°ê³¼ ì €ì¥ ì²˜ë¦¬
    payload_data = {
        "userId": str(user_id),
        "data": all_notices
    }

    # 5. ë‚´ë¶€ ì§ì ‘ í˜¸ì¶œ ì‹œë„ (ë°ë“œë½ ë°©ì§€ ë° ì„±ëŠ¥ ìµœì í™”)
    try:
        from app.main import handle_crawler_result

        # ë‚´ë¶€ í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ (await ì‚¬ìš©)
        LOG.info(f"ğŸ’¾ ë‚´ë¶€ ì €ì¥ ë¡œì§ ì§ì ‘ í˜¸ì¶œ ì‹œë„ (User: {user_id})")
        callback_obj = CallbackData(**payload_data)
        await handle_crawler_result(callback_obj)
        LOG.info("âœ… ë‚´ë¶€ ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        # ë‚´ë¶€ í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ Fallback: HTTP ìš”ì²­ (ê¸°ì¡´ ë°©ì‹)
        LOG.warning(f"ğŸ”„ ë‚´ë¶€ í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ì¸í•œ HTTP ì½œë°± ì „í™˜: {e}")
        callback_url = event.get("callbackUrl") or "http://localhost:8080/callback/save"
        try:
            response = requests.post(callback_url, json=payload_data, timeout=10)
            LOG.info(f"ğŸ“¡ HTTP ì½œë°± ê²°ê³¼: {response.status_code}")
        except Exception as http_e:
            LOG.error(f"âŒ ì½œë°± ìµœì¢… ì‹¤íŒ¨: {http_e}")

    return {"status": "SUCCESS", "count": len(all_notices)}